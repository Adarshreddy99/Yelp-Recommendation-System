{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe4df081",
   "metadata": {},
   "source": [
    "## Loading Dataset and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06da6510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading business categories...\n",
      "Loaded categories for 150,243 businesses\n",
      "Streaming reviews and counting per category...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6990280it [01:33, 74477.55it/s]                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 categories by review count:\n",
      "Restaurants: 4,724,471\n",
      "Food: 1,813,593\n",
      "Nightlife: 1,539,757\n",
      "Bars: 1,455,553\n",
      "American (Traditional): 1,011,646\n",
      "American (New): 984,540\n",
      "Breakfast & Brunch: 867,430\n",
      "Sandwiches: 691,864\n",
      "Seafood: 620,247\n",
      "Event Planning & Services: 609,553\n",
      "Shopping: 523,254\n",
      "Pizza: 475,819\n",
      "Burgers: 445,895\n",
      "Coffee & Tea: 442,348\n",
      "Italian: 439,358\n",
      "Mexican: 431,020\n",
      "Beauty & Spas: 370,121\n",
      "Arts & Entertainment: 345,059\n",
      "Cocktail Bars: 339,102\n",
      "Salad: 333,444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gzip\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "DATA_PATH = 'D:/Projects/Yelp-Recommendation-System/data/raw'\n",
    "\n",
    "# Define file paths\n",
    "business_fp = os.path.join(DATA_PATH, 'business.json')\n",
    "user_fp     = os.path.join(DATA_PATH, 'user.json')\n",
    "review_fp   = os.path.join(DATA_PATH, 'review.json')\n",
    "\n",
    "# STEP 1: Load business_id → categories mapping\n",
    "print(\"Loading business categories...\")\n",
    "\n",
    "business_categories = {}\n",
    "with open(business_fp, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        bid = data['business_id']\n",
    "        cats = data.get('categories')\n",
    "        if cats:\n",
    "            business_categories[bid] = [c.strip() for c in cats.split(',')]\n",
    "\n",
    "print(f\"Loaded categories for {len(business_categories):,} businesses\")\n",
    "\n",
    "# STEP 2: Count reviews per category by streaming review.json\n",
    "print(\"Streaming reviews and counting per category...\")\n",
    "\n",
    "category_review_count = Counter()\n",
    "\n",
    "with open(review_fp, 'r', encoding='utf-8') as f:\n",
    "    for line in tqdm(f, total=6000000):  # approx. total lines\n",
    "        data = json.loads(line)\n",
    "        bid = data['business_id']\n",
    "        cats = business_categories.get(bid)\n",
    "        if cats:\n",
    "            for cat in cats:\n",
    "                category_review_count[cat] += 1\n",
    "\n",
    "# STEP 3: Show top categories\n",
    "print(\"\\nTop 20 categories by review count:\")\n",
    "for cat, count in category_review_count.most_common(20):\n",
    "    print(f\"{cat}: {count:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66bc0cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Businesses matched: 91,610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6990280it [01:38, 70788.63it/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered reviews: 5,660,538\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define categories\n",
    "selected_categories = {'Restaurants', 'Nightlife', 'Bars', 'Shopping', 'Beauty & Spas'}\n",
    "\n",
    "# Step 1: Load businesses with selected categories\n",
    "business_meta = {}\n",
    "\n",
    "with open(business_fp, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        bid = data['business_id']\n",
    "        cats = data.get('categories')\n",
    "        city = data.get('city')\n",
    "        if cats:\n",
    "            parsed_cats = [c.strip() for c in cats.split(',')]\n",
    "            selected = [c for c in parsed_cats if c in selected_categories]\n",
    "            if selected:\n",
    "                business_meta[bid] = {\n",
    "                    'primary_category': selected[0],  # use first matched\n",
    "                    'city': city\n",
    "                }\n",
    "\n",
    "print(f\"Businesses matched: {len(business_meta):,}\")\n",
    "\n",
    "\n",
    "filtered = []\n",
    "\n",
    "with open(review_fp, 'r', encoding='utf-8') as f:\n",
    "    for line in tqdm(f, total=6990000):\n",
    "        data = json.loads(line)\n",
    "        bid = data['business_id']\n",
    "        if bid in business_meta:\n",
    "            filtered.append({\n",
    "                'user_id': data['user_id'],\n",
    "                'business_id': bid,\n",
    "                'stars': data['stars'],\n",
    "                'primary_category': business_meta[bid]['primary_category'],\n",
    "                'business_city': business_meta[bid]['city']\n",
    "            })\n",
    "\n",
    "df_filtered = pd.DataFrame(filtered)\n",
    "print(f\"Filtered reviews: {len(df_filtered):,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab9ef0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final filtered reviews: 1,252,016\n",
      "Unique users: 50,935\n",
      "Unique businesses: 19,371\n",
      "Category distribution after filtering:\n",
      "primary_category\n",
      "Restaurants      719209\n",
      "Nightlife        189260\n",
      "Bars             168344\n",
      "Shopping         112875\n",
      "Beauty & Spas     62328\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Define category-specific business thresholds\n",
    "category_thresholds = {\n",
    "    'Restaurants': 200,\n",
    "    'Nightlife': 50,\n",
    "    'Bars': 50,\n",
    "    'Shopping': 20,\n",
    "    'Beauty & Spas': 20\n",
    "}\n",
    "\n",
    "# Step 2: Filter per category using business thresholds\n",
    "filtered_parts = []\n",
    "\n",
    "for category, min_reviews in category_thresholds.items():\n",
    "    # Create a safe copy of this category's data\n",
    "    df_cat = df_filtered[df_filtered['primary_category'] == category].copy()\n",
    "    \n",
    "    # Business frequency filtering\n",
    "    business_counts = df_cat['business_id'].value_counts()\n",
    "    valid_businesses = business_counts[business_counts >= min_reviews].index\n",
    "\n",
    "    # Filter only valid businesses\n",
    "    df_cat_filtered = df_cat[df_cat['business_id'].isin(valid_businesses)].copy()\n",
    "    \n",
    "    # Store this category's filtered data\n",
    "    filtered_parts.append(df_cat_filtered)\n",
    "\n",
    "# Step 3: Combine all filtered categories\n",
    "df_combined = pd.concat(filtered_parts, ignore_index=True).copy()\n",
    "\n",
    "# Step 4: Apply global user threshold after business filtering\n",
    "MIN_USER_REVIEWS = 10\n",
    "user_counts = df_combined['user_id'].value_counts()\n",
    "valid_users = user_counts[user_counts >= MIN_USER_REVIEWS].index\n",
    "\n",
    "df_filtered_final = df_combined[df_combined['user_id'].isin(valid_users)].copy()\n",
    "\n",
    "# Step 5: Output statistics\n",
    "print(f\"Final filtered reviews: {len(df_filtered_final):,}\")\n",
    "print(f\"Unique users: {df_filtered_final['user_id'].nunique():,}\")\n",
    "print(f\"Unique businesses: {df_filtered_final['business_id'].nunique():,}\")\n",
    "print(\"Category distribution after filtering:\")\n",
    "print(df_filtered_final['primary_category'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7b65357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "_BcWyKQL16ndpBdggh2kNA    970\n",
       "ET8n-r7glWYqZhuR6GcdNw    926\n",
       "0Igx-a1wAstiBDerGxXk2A    689\n",
       "1HM81n6n4iPIFU5d2Lokhw    685\n",
       "bYENop4BuQepBjM1-BI3fA    666\n",
       "                         ... \n",
       "h4pt1pleLJfMo1XBhn-E3g     10\n",
       "OaKr_CcotOyd2e3BmfoVqg     10\n",
       "0pOWasstHx2-k4bexUOj5A     10\n",
       "wvmyHVWHWzRumGk-yuN0Fg     10\n",
       "IWixoDe6Vbyo7IRTbJj_KQ     10\n",
       "Name: count, Length: 50935, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered_final['user_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33854913",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "\n",
    "### Rationale for Category Selection\n",
    "\n",
    "The Yelp dataset contains a wide range of business types. Rather than attempting to model all categories simultaneously, we focused on a subset that is highly relevant to user interests and has sufficient review volume to support learning. The categories selected are:\n",
    "\n",
    "- Restaurants\n",
    "- Nightlife\n",
    "- Bars\n",
    "- Shopping\n",
    "- Beauty & Spas\n",
    "\n",
    "These categories are commonly interacted with by users and often contain subjective and experience-based feedback, making them particularly suitable for recommender systems.\n",
    "\n",
    "### Motivation for Category-Specific Thresholding\n",
    "\n",
    "Initial filtering based on a single threshold across all categories led to the exclusion of many useful businesses in smaller categories. To address this, we introduced category-specific filtering thresholds for businesses:\n",
    "\n",
    "- Restaurants: minimum 200 reviews per business\n",
    "- Nightlife: minimum 50 reviews\n",
    "- Bars: minimum 50 reviews\n",
    "- Shopping: minimum 20 reviews\n",
    "- Beauty & Spas: minimum 20 reviews\n",
    "\n",
    "A consistent user-level filter was applied: users must have at least 10 reviews to be included. This approach retains informative users while ensuring enough data remains in less frequently reviewed categories.\n",
    "\n",
    "### Category Grouping for Modeling\n",
    "\n",
    "For modeling purposes, the categories are grouped as follows:\n",
    "\n",
    "- Group A: Restaurants\n",
    "- Group B: Bars and NightLife - these are grouped together to increase data availability and due to similar user engagement patterns.\n",
    "- Group C: Shopping and Beauty & Spas – these are grouped together to increase data availability and due to similar user engagement patterns.\n",
    "\n",
    "This grouping balances data representation across categories and supports the design of shared or multi-task recommendation architectures.\n",
    "\n",
    "### Final Dataset Statistics\n",
    "\n",
    "After applying the filtering and grouping strategy:\n",
    "\n",
    "Final filtered reviews: 1,252,016\n",
    "Unique users: 50,935\n",
    "Unique businesses: 19,371\n",
    "\n",
    "This dataset is now well-prepared for the next steps of ID encoding, train-test splitting, and building baseline and advanced recommendation models.\n",
    "\n",
    "#### Why Business-First Then User Filtering?\n",
    "\n",
    "We chose to filter businesses first, before filtering users, to make sure we are working only with businesses that have a good number of reviews. This helps in a few important ways:\n",
    "\n",
    "1. **Remove low-quality businesses early**: Some businesses might only have a few reviews, which may not be enough for a recommendation model to learn from. Removing these early avoids wasting effort on businesses with very little data.\n",
    "\n",
    "2. **Avoid keeping users with weak interactions**: If we filtered users first, we might keep users who have written reviews for businesses that are later removed (because they were inactive). This would reduce the number of useful reviews that user has, and they might become less relevant.\n",
    "\n",
    "3. **Focus on strong user-business connections**: By filtering out weak businesses first, we make sure that when we check a user’s activity, it’s only based on reviews of good, active businesses. This gives us a clearer picture of how active and useful each user is.\n",
    "\n",
    "4. **Create a denser, higher-quality dataset**: In the final step, we keep users who have reviewed at least a minimum number of the valid businesses. This leads to a better, cleaner dataset where each user and business has enough interactions to be useful in training our models.\n",
    "\n",
    "Overall, this approach helps improve the quality of the data and ensures our recommendation model learns from strong and meaningful interactions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ab1ccb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "92058f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1252016 entries, 3 to 3453505\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Non-Null Count    Dtype  \n",
      "---  ------            --------------    -----  \n",
      " 0   user_id           1252016 non-null  object \n",
      " 1   business_id       1252016 non-null  object \n",
      " 2   stars             1252016 non-null  float64\n",
      " 3   primary_category  1252016 non-null  object \n",
      " 4   business_city     1252016 non-null  object \n",
      "dtypes: float64(1), object(4)\n",
      "memory usage: 57.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_filtered_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0515d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset saved to: D:/Projects/Yelp-Recommendation-System/data/raw\\yelp_filtered_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "## Finally save the dataset for further use and re-producability\n",
    "\n",
    "import os\n",
    "\n",
    "# Define folder path and create it if it doesn't exist\n",
    "output_dir = DATA_PATH\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = os.path.join(output_dir, 'yelp_filtered_reviews.csv')\n",
    "df_filtered_final.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Filtered dataset saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0695e889",
   "metadata": {},
   "source": [
    "## Data Pre-processing and EDA functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d294484",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torch' has no attribute 'types' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdagshub\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\Yelp-Recommendation-System\\.venv\\Lib\\site-packages\\torch\\__init__.py:2126\u001b[39m\n\u001b[32m   2119\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_compile\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _disable_dynamo  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m   2121\u001b[39m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[32m   2122\u001b[39m \u001b[38;5;66;03m# Import interface functions defined in Python\u001b[39;00m\n\u001b[32m   2123\u001b[39m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[32m   2124\u001b[39m \n\u001b[32m   2125\u001b[39m \u001b[38;5;66;03m# needs to be after the above ATen bindings so we can overwrite from Python side\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2126\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _VF \u001b[38;5;28;01mas\u001b[39;00m _VF, functional \u001b[38;5;28;01mas\u001b[39;00m functional  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m   2127\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[32m   2129\u001b[39m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[32m   2130\u001b[39m \u001b[38;5;66;03m# Remove unnecessary members\u001b[39;00m\n\u001b[32m   2131\u001b[39m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\Yelp-Recommendation-System\\.venv\\Lib\\site-packages\\torch\\functional.py:8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Optional, TYPE_CHECKING, Union\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _VF, Tensor\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _add_docstr\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\Yelp-Recommendation-System\\.venv\\Lib\\site-packages\\torch\\nn\\__init__.py:8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# mypy: allow-untyped-defs\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparameter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m      3\u001b[39m     Buffer \u001b[38;5;28;01mas\u001b[39;00m Buffer,\n\u001b[32m      4\u001b[39m     Parameter \u001b[38;5;28;01mas\u001b[39;00m Parameter,\n\u001b[32m      5\u001b[39m     UninitializedBuffer \u001b[38;5;28;01mas\u001b[39;00m UninitializedBuffer,\n\u001b[32m      6\u001b[39m     UninitializedParameter \u001b[38;5;28;01mas\u001b[39;00m UninitializedParameter,\n\u001b[32m      7\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodules\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     10\u001b[39m     attention \u001b[38;5;28;01mas\u001b[39;00m attention,\n\u001b[32m     11\u001b[39m     functional \u001b[38;5;28;01mas\u001b[39;00m functional,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     utils \u001b[38;5;28;01mas\u001b[39;00m utils,\n\u001b[32m     17\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparallel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataParallel \u001b[38;5;28;01mas\u001b[39;00m DataParallel\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\Yelp-Recommendation-System\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodule\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Module  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bilinear, Identity, LazyLinear, Linear  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      4\u001b[39m     CELU,\n\u001b[32m      5\u001b[39m     ELU,\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     Threshold,\n\u001b[32m     33\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\Yelp-Recommendation-System\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeviceLikeType\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparameter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Buffer, Parameter\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_python_dispatch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_traceable_wrapper_subclass\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhooks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackwardHook, RemovableHandle\n\u001b[32m     21\u001b[39m __all__ = [\n\u001b[32m     22\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mregister_module_forward_pre_hook\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mregister_module_forward_hook\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mModule\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     31\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\Yelp-Recommendation-System\\.venv\\Lib\\site-packages\\torch\\utils\\_python_dispatch.py:296\u001b[39m\n\u001b[32m    292\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    295\u001b[39m \u001b[38;5;66;03m# Subtypes which have __tensor_flatten__ and __tensor_unflatten__.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mTensorWithFlatten\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mProtocol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43m__tensor_flatten__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mSequence\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\Yelp-Recommendation-System\\.venv\\Lib\\site-packages\\torch\\utils\\_python_dispatch.py:335\u001b[39m, in \u001b[36mTensorWithFlatten\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdim\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m    330\u001b[39m     ...\n\u001b[32m    332\u001b[39m \u001b[38;5;129m@overload\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto\u001b[39m(\n\u001b[32m    334\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m         dtype: \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtypes\u001b[49m._dtype,\n\u001b[32m    336\u001b[39m         non_blocking: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    337\u001b[39m         copy: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    338\u001b[39m         *,\n\u001b[32m    339\u001b[39m         memory_format: Optional[torch.memory_format] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    340\u001b[39m ) -> torch.Tensor:\n\u001b[32m    341\u001b[39m     ...\n\u001b[32m    343\u001b[39m \u001b[38;5;129m@overload\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto\u001b[39m(\n\u001b[32m    345\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    351\u001b[39m         memory_format: Optional[torch.memory_format] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    352\u001b[39m ) -> torch.Tensor:\n",
      "\u001b[31mAttributeError\u001b[39m: partially initialized module 'torch' has no attribute 'types' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import mlflow\n",
    "import dagshub\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import faiss\n",
    "from collections import defaultdict\n",
    "import scipy.sparse as sp\n",
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "# Defines all essential parameters for the pipeline,\n",
    "# including data paths, MLflow settings, model hyperparameters,\n",
    "# and training specifics.\n",
    "CONFIG = {\n",
    "    \"csv_path\": \"D:/Projects/Yelp-Recomendation-System/data/raw/yelp_filtered_reviews.csv\", # Path to the raw review data CSV\n",
    "    \"mlflow_tracking_uri\": \"https://dagshub.com/Adarshreddy99/Yelp-Recommendation-System.mlflow\", # MLflow tracking server URI\n",
    "    \"dagshub_repo_owner\": \"Adarshreddy99\", # DagsHub repository owner\n",
    "    \"dagshub_repo_name\": \"Yelp-Recommendation-System\", # DagsHub repository name\n",
    "    \"experiment_name\": \"Hybrid_Recommendation_System\", # Name of the MLflow experiment\n",
    "    \"embedding_dim\": 64,    # Dimension of user and item embeddings\n",
    "    \"hidden_dim\": 128,      # Dimension of hidden layers in networks\n",
    "    \"batch_size\": 1024,     # Batch size for training\n",
    "    \"learning_rate\": 0.001, # Learning rate for optimizer\n",
    "    \"epochs\": 20,           # Number of training epochs\n",
    "    \"test_size\": 0.2,       # Proportion of data for the test set\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Device to run training (GPU if available, else CPU)\n",
    "}\n",
    "\n",
    "# ============= MLflow Setup =============\n",
    "def setup_mlflow():\n",
    "    \"\"\"\n",
    "    Sets up MLflow tracking by defining the tracking URI and initializing DagsHub\n",
    "    for seamless integration with MLflow.\n",
    "    \"\"\"\n",
    "    mlflow.set_tracking_uri(CONFIG[\"mlflow_tracking_uri\"])\n",
    "    # Initialize DagsHub for MLflow integration\n",
    "    dagshub.init(repo_owner='Adarshreddy99', repo_name='Yelp-Recommendation-System', mlflow=True)\n",
    "    mlflow.set_experiment(CONFIG[\"experiment_name\"])\n",
    "    print(\"MLflow setup complete\")\n",
    "\n",
    "# ============= Data Loading & Preprocessing =============\n",
    "\n",
    "def load_data(file_path_on_drive):\n",
    "    \"\"\"\n",
    "    Mounts Google Drive, loads the dataset from the given path, and performs initial data validation.\n",
    "    Ensures required columns are present and handles missing values.\n",
    "\n",
    "    Args:\n",
    "        file_path_on_drive (str): The relative file path within Google Drive (e.g., 'MyDrive/data/file.csv').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The loaded and initially validated DataFrame.\n",
    "    \"\"\"\n",
    "    # Mount Google Drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    full_path = os.path.join('/content/drive', file_path_on_drive)\n",
    "    print(f\"Loading data from: {full_path}\")\n",
    "    \n",
    "    df = pd.read_csv(full_path)\n",
    "    print(f\"Loaded data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    \n",
    "    # Basic validation: Check for essential columns\n",
    "    required_cols = ['user_id', 'business_id', 'stars']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    # Remove rows with null values in essential columns\n",
    "    initial_rows = len(df)\n",
    "    df = df.dropna(subset=required_cols)\n",
    "    print(f\"Removed {initial_rows - len(df)} rows with nulls in essential columns. After cleaning: {df.shape}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def categorize_businesses(df):\n",
    "    \"\"\"\n",
    "    Categorizes businesses into predefined main categories (Restaurants, Bars/Nightlife, Shopping/Beauty).\n",
    "    It looks for keywords in a 'categories' column. If the column is missing, it assigns random\n",
    "    categories for demonstration.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing business data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with a new 'category' column and 'Other' categories filtered out.\n",
    "    \"\"\"\n",
    "    # Define keywords for each category\n",
    "    restaurant_keywords = ['restaurant', 'food', 'pizza', 'burger', 'cafe', 'diner', 'bistro', 'steak', 'sushi', 'deli']\n",
    "    nightlife_keywords = ['bar', 'pub', 'nightlife', 'club', 'lounge', 'brewery', 'cocktail', 'wine']\n",
    "    beauty_shopping_keywords = ['beauty', 'spa', 'salon', 'shopping', 'retail', 'boutique', 'fashion', 'store', 'mall', 'hair']\n",
    "    \n",
    "    def assign_category(categories_str):\n",
    "        \"\"\"Helper function to assign a single category based on keywords.\"\"\"\n",
    "        if pd.isna(categories_str):\n",
    "            return 'Other'\n",
    "        \n",
    "        categories_lower = str(categories_str).lower()\n",
    "        \n",
    "        if any(keyword in categories_lower for keyword in restaurant_keywords):\n",
    "            return 'Restaurants'\n",
    "        elif any(keyword in categories_lower for keyword in nightlife_keywords):\n",
    "            return 'BarsNightlife'\n",
    "        elif any(keyword in categories_lower for keyword in beauty_shopping_keywords):\n",
    "            return 'ShoppingBeauty'\n",
    "        else:\n",
    "            return 'Other'\n",
    "    \n",
    "    # Apply categorization if 'categories' column exists\n",
    "    if 'categories' in df.columns:\n",
    "        df['category'] = df['categories'].apply(assign_category)\n",
    "    else:\n",
    "        # Fallback for demonstration if 'categories' column is not found\n",
    "        print(\"Warning: No 'categories' column found. Randomly assigning categories for demo purposes.\")\n",
    "        categories = ['Restaurants', 'BarsNightlife', 'ShoppingBeauty']\n",
    "        df['category'] = np.random.choice(categories, size=len(df))\n",
    "    \n",
    "    # Filter out businesses classified as 'Other' to focus on relevant categories\n",
    "    initial_rows = len(df)\n",
    "    df = df[df['category'] != 'Other'].copy()\n",
    "    print(f\"Filtered out {initial_rows - len(df)} 'Other' category businesses. After categorization: {df.shape}\")\n",
    "    print(f\"Category distribution:\\n{df['category'].value_counts()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_mappings(df):\n",
    "    \"\"\"\n",
    "    Creates unique integer mappings for users and category-specific items.\n",
    "    This is essential for embedding layers in neural networks.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame with 'user_id', 'business_id', and 'category' columns.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - pd.DataFrame: The DataFrame with 'user_idx' and 'item_idx' columns.\n",
    "            - dict: A mapping from original user_id to integer user_idx.\n",
    "            - dict: A dictionary of category-specific item mappings (original item_id to integer item_idx).\n",
    "    \"\"\"\n",
    "    # Create a global mapping for all unique users\n",
    "    user2idx = {u: i for i, u in enumerate(df['user_id'].unique())}\n",
    "    \n",
    "    # Create category-specific mappings for items (businesses)\n",
    "    item_mappings = {}\n",
    "    for category in df['category'].unique():\n",
    "        cat_items = df[df['category'] == category]['business_id'].unique()\n",
    "        item_mappings[category] = {item: i for i, item in enumerate(cat_items)}\n",
    "    \n",
    "    # Apply the created mappings to the DataFrame\n",
    "    df['user_idx'] = df['user_id'].map(user2idx)\n",
    "    \n",
    "    # Apply category-specific item mappings\n",
    "    def map_item_idx(row):\n",
    "        \"\"\"Helper function to map item_id based on its category.\"\"\"\n",
    "        # Use .get() with a default value of -1 to identify items not found in mapping\n",
    "        return item_mappings[row['category']].get(row['business_id'], -1)\n",
    "    \n",
    "    df['item_idx'] = df.apply(map_item_idx, axis=1)\n",
    "    \n",
    "    # Remove any rows where mapping failed (e.g., business_id not found in its category's mapping)\n",
    "    initial_rows = len(df)\n",
    "    df = df[df['item_idx'] != -1].copy()\n",
    "    print(f\"Removed {initial_rows - len(df)} rows due to mapping failures. Final data shape: {df.shape}\")\n",
    "    print(f\"Total number of unique users: {len(user2idx)}\")\n",
    "    for cat, mapping in item_mappings.items():\n",
    "        print(f\"Total number of unique items in '{cat}': {len(mapping)}\")\n",
    "    \n",
    "    return df, user2idx, item_mappings\n",
    "\n",
    "def split_data(df, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Splits the preprocessed DataFrame into training and testing sets.\n",
    "    Stratifies the split by 'category' to ensure balanced category distribution in both sets.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame with 'category' column.\n",
    "        test_size (float): The proportion of the dataset to include in the test split.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the training and testing DataFrames.\n",
    "    \"\"\"\n",
    "    train_df, test_df = train_test_split(\n",
    "        df, \n",
    "        test_size=test_size, \n",
    "        stratify=df['category'], # Ensure category distribution is preserved\n",
    "        random_state=42 # For reproducibility\n",
    "    )\n",
    "    \n",
    "    print(f\"Train set size: {train_df.shape}\")\n",
    "    print(f\"Test set size: {test_df.shape}\")\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "# ============= Exploratory Data Analysis (EDA) Functions =============\n",
    "def perform_eda(df):\n",
    "    \"\"\"\n",
    "    Performs comprehensive Exploratory Data Analysis (EDA) on the DataFrame.\n",
    "    Generates and saves various plots (rating distribution, category distribution, user/item activity).\n",
    "    Logs these plots as artifacts to MLflow.\n",
    "    Calculates and prints sparsity metrics for each category.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame after preprocessing.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing sparsity metrics for each category.\n",
    "    \"\"\"\n",
    "    print(\"Starting EDA...\")\n",
    "\n",
    "    # 1. Rating Distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df['stars'], bins=5, kde=True, color='skyblue')\n",
    "    plt.title(\"Distribution of Ratings (Stars)\", fontsize=16)\n",
    "    plt.xlabel(\"Stars Rating\", fontsize=12)\n",
    "    plt.ylabel(\"Frequency\", fontsize=12)\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.xticks([1, 2, 3, 4, 5])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"rating_distribution.png\", dpi=300, bbox_inches='tight')\n",
    "    mlflow.log_artifact(\"rating_distribution.png\")\n",
    "    plt.close() # Close plot to free memory\n",
    "    print(\"Logged 'rating_distribution.png'\")\n",
    "    \n",
    "    # 2. Category Distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df['category'].value_counts().plot(kind='bar', color='lightcoral')\n",
    "    plt.title(\"Distribution of Business Categories\", fontsize=16)\n",
    "    plt.xlabel(\"Category\", fontsize=12)\n",
    "    plt.ylabel(\"Number of Reviews\", fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right') # Rotate labels for better readability\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"category_distribution.png\", dpi=300, bbox_inches='tight')\n",
    "    mlflow.log_artifact(\"category_distribution.png\")\n",
    "    plt.close()\n",
    "    print(\"Logged 'category_distribution.png'\")\n",
    "    \n",
    "    # 3. User and Item Activity Distributions\n",
    "    user_counts = df['user_idx'].value_counts()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12)) # Adjusted for better spacing\n",
    "    \n",
    "    # User activity distribution (linear scale)\n",
    "    sns.histplot(user_counts, bins=50, ax=axes[0,0], color='lightgreen')\n",
    "    axes[0,0].set_title(\"User Activity Distribution (Linear Scale)\", fontsize=14)\n",
    "    axes[0,0].set_xlabel(\"Number of Reviews per User\", fontsize=11)\n",
    "    axes[0,0].set_ylabel(\"Number of Users\", fontsize=11)\n",
    "    \n",
    "    # User activity distribution (log scale)\n",
    "    sns.histplot(user_counts, bins=50, ax=axes[0,1], color='lightgreen')\n",
    "    axes[0,1].set_yscale('log') # Logarithmic scale for frequency\n",
    "    axes[0,1].set_title(\"User Activity Distribution (Log Scale)\", fontsize=14)\n",
    "    axes[0,1].set_xlabel(\"Number of Reviews per User\", fontsize=11)\n",
    "    axes[0,1].set_ylabel(\"Number of Users (Log Scale)\", fontsize=11)\n",
    "    \n",
    "    # Item activity by category\n",
    "    colors = ['cornflowerblue', 'orange', 'purple'] # Define specific colors for categories\n",
    "    for i, category in enumerate(['Restaurants', 'BarsNightlife', 'ShoppingBeauty']):\n",
    "        if category in df['category'].values: # Check if category exists in the data\n",
    "            cat_data = df[df['category'] == category]\n",
    "            item_counts = cat_data['item_idx'].value_counts()\n",
    "            \n",
    "            # Dynamically assign subplot for item activity\n",
    "            if i == 0: # First category to axes[1,0]\n",
    "                ax = axes[1,0]\n",
    "            elif i == 1: # Second category to axes[1,1]\n",
    "                ax = axes[1,1]\n",
    "            else: # Third category might need a new subplot or handle differently if more than 2x2 layout\n",
    "                # For this specific 2x2 layout, if there are more than 2 categories\n",
    "                # for the bottom row, it might overlap or need a different subplot layout.\n",
    "                # Assuming max 2 for bottom row or will handle dynamically.\n",
    "                # For simplicity with 3 categories, one might be placed in the 2nd row, 1st col, next in 2nd row, 2nd col.\n",
    "                # If a third one exists, it would be placed in a new subplot outside the initial 2x2 grid,\n",
    "                # which can lead to layout issues. A better way would be fig.add_subplot(2,3,i+1+3) for a 2x3 grid,\n",
    "                # but we'll stick to the original structure for now.\n",
    "                # Let's assign the third category to the second subplot of the second row,\n",
    "                # if there are only 2 main categories shown on row 2, and overlay them.\n",
    "                # Or, if this function is only called once and three items are there,\n",
    "                # the third category data won't be plotted.\n",
    "                # To ensure it plots, let's adjust for a specific 2x2 layout or ensure\n",
    "                # the loop only iterates over relevant categories.\n",
    "                pass # This part might need explicit subplot creation if >2 categories for the last row\n",
    "            \n",
    "            sns.histplot(item_counts, bins=30, ax=ax, alpha=0.7, label=category, color=colors[i % len(colors)])\n",
    "            ax.set_title(f\"{category} Item Activity\", fontsize=14)\n",
    "            ax.set_xlabel(\"Number of Reviews per Item\", fontsize=11)\n",
    "            ax.set_ylabel(\"Number of Items\", fontsize=11)\n",
    "            ax.legend()\n",
    "\n",
    "    plt.tight_layout() # Adjust subplot parameters for a tight layout\n",
    "    plt.savefig(\"activity_distributions.png\", dpi=300, bbox_inches='tight')\n",
    "    mlflow.log_artifact(\"activity_distributions.png\")\n",
    "    plt.close()\n",
    "    print(\"Logged 'activity_distributions.png'\")\n",
    "    \n",
    "    # 4. Calculate and log sparsity metrics\n",
    "    sparsity_metrics = {}\n",
    "    print(\"\\nCalculating data sparsity:\")\n",
    "    for category in df['category'].unique():\n",
    "        cat_data = df[df['category'] == category]\n",
    "        n_users_cat = cat_data['user_idx'].nunique()\n",
    "        n_items_cat = cat_data['item_idx'].nunique()\n",
    "        n_ratings_cat = len(cat_data)\n",
    "        \n",
    "        # Avoid division by zero if n_users or n_items is 0 for a category\n",
    "        if n_users_cat > 0 and n_items_cat > 0:\n",
    "            sparsity = 1 - (n_ratings_cat / (n_users_cat * n_items_cat))\n",
    "            sparsity_metrics[f'{category}_sparsity'] = sparsity\n",
    "            print(f\"  Category '{category}':\")\n",
    "            print(f\"    Unique Users: {n_users_cat}, Unique Items: {n_items_cat}, Total Ratings: {n_ratings_cat}\")\n",
    "            print(f\"    Sparsity: {sparsity:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Category '{category}': No data or no unique users/items found.\")\n",
    "            sparsity_metrics[f'{category}_sparsity'] = 1.0 # Fully sparse if no interactions\n",
    "            \n",
    "    print(\"EDA complete.\")\n",
    "    return sparsity_metrics\n",
    "\n",
    "# ============= Model Architecture =============\n",
    "class UserNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network for generating user embeddings.\n",
    "    It takes user IDs as input, passes them through an embedding layer,\n",
    "    and then through a fully connected network with BatchNorm and ReLU activation.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_users, emb_dim=64, hidden_dim=128):\n",
    "        \"\"\"\n",
    "        Initializes the UserNetwork.\n",
    "\n",
    "        Args:\n",
    "            num_users (int): Total number of unique users.\n",
    "            emb_dim (int): Desired dimension of the user embedding.\n",
    "            hidden_dim (int): Dimension of the hidden layer in the FC block.\n",
    "        \"\"\"\n",
    "        super(UserNetwork, self).__init__()\n",
    "        # Embedding layer for users\n",
    "        self.embedding = nn.Embedding(num_users, emb_dim)\n",
    "        # Fully connected layers to process embeddings\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(emb_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim), # Batch Normalization for stability\n",
    "            nn.ReLU(),                  # ReLU activation\n",
    "            nn.Dropout(0.2),            # Dropout for regularization\n",
    "            nn.Linear(hidden_dim, emb_dim) # Output layer to the embedding dimension\n",
    "        )\n",
    "        \n",
    "    def forward(self, user_ids):\n",
    "        \"\"\"\n",
    "        Forward pass for the UserNetwork.\n",
    "\n",
    "        Args:\n",
    "            user_ids (torch.Tensor): Tensor of user indices.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: User embeddings.\n",
    "        \"\"\"\n",
    "        x = self.embedding(user_ids)\n",
    "        return self.fc(x)\n",
    "\n",
    "class ItemNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network for generating item embeddings.\n",
    "    Similar to UserNetwork but for items, allowing category-specific item representations.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_items, emb_dim=64, hidden_dim=128):\n",
    "        \"\"\"\n",
    "        Initializes the ItemNetwork.\n",
    "\n",
    "        Args:\n",
    "            num_items (int): Total number of unique items for a given category.\n",
    "            emb_dim (int): Desired dimension of the item embedding.\n",
    "            hidden_dim (int): Dimension of the hidden layer in the FC block.\n",
    "        \"\"\"\n",
    "        super(ItemNetwork, self).__init__()\n",
    "        # Embedding layer for items\n",
    "        self.embedding = nn.Embedding(num_items, emb_dim)\n",
    "        # Fully connected layers to process embeddings\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(emb_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, emb_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, item_ids):\n",
    "        \"\"\"\n",
    "        Forward pass for the ItemNetwork.\n",
    "\n",
    "        Args:\n",
    "            item_ids (torch.Tensor): Tensor of item indices.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Item embeddings.\n",
    "        \"\"\"\n",
    "        x = self.embedding(item_ids)\n",
    "        return self.fc(x)\n",
    "\n",
    "class HybridRecommendationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The main hybrid recommendation model combining a shared user network\n",
    "    and category-specific item networks. It predicts ratings based on the\n",
    "    interaction between user and item embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_users, category_item_counts, emb_dim=64, hidden_dim=128):\n",
    "        \"\"\"\n",
    "        Initializes the HybridRecommendationModel.\n",
    "\n",
    "        Args:\n",
    "            num_users (int): Total number of unique users across all categories.\n",
    "            category_item_counts (dict): A dictionary where keys are category names\n",
    "                                         and values are the number of unique items in that category.\n",
    "            emb_dim (int): Dimension of user and item embeddings.\n",
    "            hidden_dim (int): Dimension of hidden layers in the networks.\n",
    "        \"\"\"\n",
    "        super(HybridRecommendationModel, self).__init__()\n",
    "        \n",
    "        # Shared user network for all users\n",
    "        self.user_net = UserNetwork(num_users, emb_dim, hidden_dim)\n",
    "        \n",
    "        # ModuleDict to hold category-specific item networks\n",
    "        # This allows dynamic access to the correct item network based on category\n",
    "        self.item_nets = nn.ModuleDict({\n",
    "            category: ItemNetwork(count, emb_dim, hidden_dim)\n",
    "            for category, count in category_item_counts.items()\n",
    "        })\n",
    "        \n",
    "        # Output layer for rating prediction (maps interaction to a single rating value)\n",
    "        self.rating_predictor = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 32), # Intermediate linear layer\n",
    "            nn.ReLU(),              # Activation\n",
    "            nn.Dropout(0.1),        # Dropout\n",
    "            nn.Linear(32, 1),       # Output to a single scalar\n",
    "            nn.Sigmoid()            # Sigmoid to normalize output between 0 and 1\n",
    "        )\n",
    "        \n",
    "    def forward(self, user_ids, item_ids, category):\n",
    "        \"\"\"\n",
    "        Forward pass for the HybridRecommendationModel.\n",
    "\n",
    "        Args:\n",
    "            user_ids (torch.Tensor): Tensor of user indices.\n",
    "            item_ids (torch.Tensor): Tensor of item indices (specific to the given category).\n",
    "            category (str): The category name (e.g., 'Restaurants', 'BarsNightlife').\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - torch.Tensor: Predicted ratings (scaled to 1-5).\n",
    "                - torch.Tensor: User embeddings.\n",
    "                - torch.Tensor: Item embeddings (from the specific category network).\n",
    "        \"\"\"\n",
    "        # Get user embeddings from the shared user network\n",
    "        user_emb = self.user_net(user_ids)\n",
    "        \n",
    "        # Get category-specific item embeddings using the correct item network\n",
    "        # The category argument selects which item network to use from ModuleDict\n",
    "        item_emb = self.item_nets[category](item_ids)\n",
    "        \n",
    "        # Compute interaction between user and item embeddings (e.g., element-wise product)\n",
    "        interaction = user_emb * item_emb\n",
    "        \n",
    "        # Predict rating using the rating predictor head\n",
    "        # Scale the sigmoid output (0-1) to the 1-5 rating scale\n",
    "        rating = self.rating_predictor(interaction).squeeze() * 4 + 1\n",
    "        \n",
    "        return rating, user_emb, item_emb\n",
    "\n",
    "# ============= Dataset and DataLoader =============\n",
    "class RecommendationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for handling recommendation data.\n",
    "    It wraps a pandas DataFrame and provides data for training/evaluation.\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Initializes the RecommendationDataset.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The DataFrame containing 'user_idx', 'item_idx', 'stars', and 'category'.\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True) # Reset index to ensure integer-based indexing\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a single sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing 'user_id', 'item_id', 'rating', and 'category' for the sample.\n",
    "        \"\"\"\n",
    "        row = self.df.iloc[idx]\n",
    "        return {\n",
    "            'user_id': row['user_idx'],\n",
    "            'item_id': row['item_idx'],\n",
    "            'rating': row['stars'],\n",
    "            'category': row['category']\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for DataLoader.\n",
    "    It groups samples by category, which is necessary because each category\n",
    "    uses a different item embedding network in the model.\n",
    "\n",
    "    Args:\n",
    "        batch (list): A list of samples (dictionaries) from the RecommendationDataset.\n",
    "\n",
    "    Returns:\n",
    "        defaultdict: A dictionary where keys are categories and values are lists of samples\n",
    "                     belonging to that category.\n",
    "    \"\"\"\n",
    "    category_batches = defaultdict(list)\n",
    "    \n",
    "    for item in batch:\n",
    "        category_batches[item['category']].append(item)\n",
    "    \n",
    "    return category_batches\n",
    "\n",
    "# ============= Training Functions =============\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Performs one epoch of training for the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The recommendation model.\n",
    "        dataloader (DataLoader): DataLoader for the training data.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for model parameters.\n",
    "        criterion (nn.Module): Loss function (e.g., MSELoss).\n",
    "        device (torch.device): The device (CPU/GPU) to run the training on.\n",
    "\n",
    "    Returns:\n",
    "        float: The average training loss for the epoch.\n",
    "    \"\"\"\n",
    "    model.train() # Set model to training mode\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_dict in dataloader:\n",
    "        optimizer.zero_grad() # Clear gradients from previous step\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # Iterate through each category in the batch (grouped by collate_fn)\n",
    "        for category, items in batch_dict.items():\n",
    "            if not items:\n",
    "                continue # Skip if no items for this category in the current batch\n",
    "            \n",
    "            # Convert list of dicts to tensors\n",
    "            user_ids = torch.tensor([item['user_id'] for item in items], \n",
    "                                    dtype=torch.long, device=device)\n",
    "            item_ids = torch.tensor([item['item_id'] for item in items], \n",
    "                                    dtype=torch.long, device=device)\n",
    "            ratings = torch.tensor([item['rating'] for item in items], \n",
    "                                   dtype=torch.float, device=device)\n",
    "            \n",
    "            # Forward pass: get predicted ratings\n",
    "            pred_ratings, _, _ = model(user_ids, item_ids, category)\n",
    "            \n",
    "            # Compute loss for this category and add to epoch loss\n",
    "            loss = criterion(pred_ratings, ratings)\n",
    "            epoch_loss += loss\n",
    "        \n",
    "        # Backward pass and optimize if there was any loss accumulated\n",
    "        if epoch_loss > 0:\n",
    "            epoch_loss.backward() # Accumulate gradients\n",
    "            optimizer.step()      # Update model parameters\n",
    "            total_loss += epoch_loss.item() # Add loss to total\n",
    "            num_batches += 1\n",
    "    \n",
    "    # Return average loss, handling cases where num_batches might be zero\n",
    "    return total_loss / max(num_batches, 1)\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on a given dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The recommendation model.\n",
    "        dataloader (DataLoader): DataLoader for the evaluation data.\n",
    "        device (torch.device): The device (CPU/GPU) to run the evaluation on.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing overall and category-specific evaluation metrics (MSE, MAE, RMSE).\n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    category_metrics = defaultdict(list) # To store predictions and actuals per category\n",
    "    \n",
    "    with torch.no_grad(): # Disable gradient calculation for evaluation\n",
    "        for batch_dict in dataloader:\n",
    "            for category, items in batch_dict.items():\n",
    "                if not items:\n",
    "                    continue\n",
    "                    \n",
    "                user_ids = torch.tensor([item['user_id'] for item in items], \n",
    "                                        dtype=torch.long, device=device)\n",
    "                item_ids = torch.tensor([item['item_id'] for item in items], \n",
    "                                        dtype=torch.long, device=device)\n",
    "                ratings = torch.tensor([item['rating'] for item in items], \n",
    "                                       dtype=torch.float, device=device)\n",
    "                \n",
    "                pred_ratings, _, _ = model(user_ids, item_ids, category)\n",
    "                \n",
    "                # Collect overall predictions and actuals\n",
    "                predictions.extend(pred_ratings.cpu().numpy())\n",
    "                actuals.extend(ratings.cpu().numpy())\n",
    "                \n",
    "                # Collect category-specific predictions and actuals\n",
    "                category_metrics[category].extend(list(zip(\n",
    "                    pred_ratings.cpu().numpy(), \n",
    "                    ratings.cpu().numpy()\n",
    "                )))\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Calculate category-specific metrics\n",
    "    cat_metrics = {}\n",
    "    for category, preds_actual in category_metrics.items():\n",
    "        if preds_actual: # Ensure there are predictions for this category\n",
    "            cat_preds, cat_actuals = zip(*preds_actual) # Unzip predictions and actuals\n",
    "            cat_metrics[f'{category}_mse'] = mean_squared_error(cat_actuals, cat_preds)\n",
    "            cat_metrics[f'{category}_mae'] = mean_absolute_error(cat_actuals, cat_preds)\n",
    "            cat_metrics[f'{category}_rmse'] = np.sqrt(cat_metrics[f'{category}_mse'])\n",
    "        else:\n",
    "            print(f\"Warning: No predictions for category '{category}' in evaluation.\")\n",
    "    \n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        **cat_metrics # Unpack category-specific metrics into the main dictionary\n",
    "    }\n",
    "\n",
    "# ============= FAISS Index Functions =============\n",
    "def build_faiss_indices(model, item_mappings, device):\n",
    "    \"\"\"\n",
    "    Builds FAISS (Facebook AI Similarity Search) indices for each category.\n",
    "    This allows for efficient nearest-neighbor search for item recommendations.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained recommendation model.\n",
    "        item_mappings (dict): Dictionary of category-specific item mappings.\n",
    "        device (torch.device): The device (CPU/GPU) the model is on.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are category names and values are the corresponding FAISS indices.\n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    indices = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for category, item_mapping in item_mappings.items():\n",
    "            n_items = len(item_mapping)\n",
    "            # Create a tensor of item indices for the current category\n",
    "            item_ids = torch.arange(n_items, dtype=torch.long, device=device)\n",
    "            \n",
    "            embeddings = []\n",
    "            batch_size = 1024 # Process items in batches to manage memory\n",
    "            \n",
    "            # Extract item embeddings in batches\n",
    "            for i in range(0, n_items, batch_size):\n",
    "                batch_ids = item_ids[i:i+batch_size]\n",
    "                # To get item embeddings, we need to pass dummy user_ids and category\n",
    "                # The user_ids are not used by the item_net directly, just for the forward signature\n",
    "                _, _, item_emb = model(\n",
    "                    torch.zeros_like(batch_ids), # Dummy user_ids (values don't matter here)\n",
    "                    batch_ids, \n",
    "                    category\n",
    "                )\n",
    "                embeddings.append(item_emb.cpu().numpy())\n",
    "            \n",
    "            # Concatenate all collected embeddings\n",
    "            if embeddings: # Check if embeddings list is not empty\n",
    "                all_embeddings = np.vstack(embeddings)\n",
    "                \n",
    "                # Build FAISS index: IndexFlatL2 for L2 distance (Euclidean)\n",
    "                # D = embedding dimension\n",
    "                index = faiss.IndexFlatL2(all_embeddings.shape[1]) \n",
    "                index.add(all_embeddings.astype('float32')) # Add embeddings to the index\n",
    "                indices[category] = index\n",
    "                \n",
    "                print(f\"Built FAISS index for '{category}' with {all_embeddings.shape[0]} items and {all_embeddings.shape[1]} dimensions.\")\n",
    "            else:\n",
    "                print(f\"No embeddings generated for category '{category}'. Skipping FAISS index creation.\")\n",
    "    \n",
    "    return indices\n",
    "\n",
    "def get_recommendations(model, user_id, category, faiss_index, item_mappings, \n",
    "                        device, k=10):\n",
    "    \"\"\"\n",
    "    Retrieves top-k item recommendations for a given user within a specific category\n",
    "    using the trained model and FAISS index.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained recommendation model.\n",
    "        user_id (int): The mapped integer ID of the user.\n",
    "        category (str): The category for which to retrieve recommendations.\n",
    "        faiss_index (faiss.Index): The FAISS index for the specified category.\n",
    "        item_mappings (dict): The dictionary of category-specific item mappings (to map back item_idx to original item_id).\n",
    "        device (torch.device): The device (CPU/GPU) the model is on.\n",
    "        k (int): The number of top recommendations to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - np.array: Distances to the recommended items.\n",
    "            - np.array: Mapped item indices of the recommended items.\n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get the user embedding from the model\n",
    "        user_tensor = torch.tensor([user_id], dtype=torch.long, device=device)\n",
    "        # Dummy item_id is needed for the forward pass, but its value doesn't affect user_emb\n",
    "        dummy_item = torch.tensor([0], dtype=torch.long, device=device) \n",
    "        \n",
    "        _, user_emb, _ = model(user_tensor, dummy_item, category)\n",
    "        user_emb_np = user_emb.cpu().numpy().astype('float32') # Convert to numpy for FAISS\n",
    "        \n",
    "        # Search for nearest neighbors (items) to the user embedding in the FAISS index\n",
    "        distances, item_indices = faiss_index.search(user_emb_np, k)\n",
    "        \n",
    "        return distances[0], item_indices[0] # Return the first (and only) user's results\n",
    "\n",
    "# ============= Main Pipeline Execution =============\n",
    "def main_pipeline():\n",
    "    \"\"\"\n",
    "    Orchestrates the entire hybrid recommendation system pipeline:\n",
    "    1. Sets up MLflow tracking.\n",
    "    2. Loads and preprocesses data.\n",
    "    3. Performs EDA and logs results.\n",
    "    4. Splits data into train/test sets.\n",
    "    5. Initializes the PyTorch model.\n",
    "    6. Trains the model, logging metrics per epoch.\n",
    "    7. Evaluates the trained model.\n",
    "    8. Builds and saves FAISS indices.\n",
    "    9. Demonstrates sample recommendations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start an MLflow run to log all aspects of this pipeline execution\n",
    "    with mlflow.start_run():\n",
    "        print(\"Starting Hybrid Recommendation System Pipeline...\")\n",
    "        \n",
    "        # Log all configuration parameters for reproducibility\n",
    "        mlflow.log_params(CONFIG)\n",
    "        print(\"Configuration parameters logged to MLflow.\")\n",
    "        \n",
    "        # 1. Load and preprocess data\n",
    "        print(\"\\n1. Loading and preprocessing data...\")\n",
    "        try:\n",
    "            df = load_data('MyDrive/yelp_filtered_reviews.csv')\n",
    "            df = categorize_businesses(df)\n",
    "            df, user2idx, item_mappings = create_mappings(df)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error during data loading/preprocessing: {e}\")\n",
    "            mlflow.log_param(\"data_load_status\", \"failed\")\n",
    "            return None, None, None # Exit pipeline if data loading fails\n",
    "        \n",
    "        # Log key data statistics to MLflow\n",
    "        mlflow.log_metric(\"total_users_after_mapping\", len(user2idx))\n",
    "        mlflow.log_metric(\"total_interactions_after_mapping\", len(df))\n",
    "        for category, mapping in item_mappings.items():\n",
    "            mlflow.log_metric(f\"{category}_total_items\", len(mapping))\n",
    "        print(\"Data loaded, preprocessed, and mappings created.\")\n",
    "        \n",
    "        # 2. Perform EDA\n",
    "        print(\"\\n2. Performing EDA...\")\n",
    "        sparsity_metrics = perform_eda(df)\n",
    "        mlflow.log_metrics(sparsity_metrics) # Log sparsity metrics\n",
    "        print(\"EDA complete and plots logged to MLflow.\")\n",
    "        \n",
    "        # 3. Split data into training and testing sets\n",
    "        print(\"\\n3. Splitting data into train and test sets...\")\n",
    "        train_df, test_df = split_data(df, CONFIG[\"test_size\"])\n",
    "        mlflow.log_metric(\"train_data_size\", len(train_df))\n",
    "        mlflow.log_metric(\"test_data_size\", len(test_df))\n",
    "        print(\"Data split complete.\")\n",
    "        \n",
    "        # 4. Create PyTorch datasets and dataloaders\n",
    "        print(\"\\n4. Creating PyTorch datasets and dataloaders...\")\n",
    "        train_dataset = RecommendationDataset(train_df)\n",
    "        test_dataset = RecommendationDataset(test_df)\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=CONFIG[\"batch_size\"], \n",
    "            shuffle=True, # Shuffle training data\n",
    "            collate_fn=collate_fn, # Use custom collate function for category grouping\n",
    "            pin_memory=True if CONFIG[\"device\"].type == 'cuda' else False # Pin memory for faster GPU transfer\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=CONFIG[\"batch_size\"], \n",
    "            shuffle=False, # No need to shuffle test data\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True if CONFIG[\"device\"].type == 'cuda' else False\n",
    "        )\n",
    "        print(\"Datasets and DataLoaders created.\")\n",
    "        \n",
    "        # 5. Initialize the Hybrid Recommendation Model\n",
    "        print(\"\\n5. Initializing the model...\")\n",
    "        category_item_counts = {cat: len(mapping) for cat, mapping in item_mappings.items()}\n",
    "        \n",
    "        model = HybridRecommendationModel(\n",
    "            num_users=len(user2idx),\n",
    "            category_item_counts=category_item_counts,\n",
    "            emb_dim=CONFIG[\"embedding_dim\"],\n",
    "            hidden_dim=CONFIG[\"hidden_dim\"]\n",
    "        ).to(CONFIG[\"device\"]) # Move model to the specified device\n",
    "        \n",
    "        # Log model parameters to MLflow\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        mlflow.log_metric(\"model_total_parameters\", total_params)\n",
    "        print(f\"Model initialized with {total_params:,} trainable parameters.\")\n",
    "        \n",
    "        # 6. Set up training components: optimizer and loss function\n",
    "        print(\"\\n6. Setting up training optimizer and loss function...\")\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "        criterion = nn.MSELoss() # Mean Squared Error Loss for regression\n",
    "        print(\"Optimizer (Adam) and Criterion (MSELoss) set up.\")\n",
    "        \n",
    "        # 7. Training loop\n",
    "        print(\"\\n7. Starting model training...\")\n",
    "        best_val_rmse = float('inf') # Track best validation RMSE for saving model\n",
    "        \n",
    "        for epoch in range(CONFIG[\"epochs\"]):\n",
    "            # Train for one epoch\n",
    "            train_loss = train_epoch(model, train_loader, optimizer, criterion, CONFIG[\"device\"])\n",
    "            \n",
    "            # Evaluate model periodically and at the last epoch\n",
    "            if epoch % 5 == 0 or epoch == CONFIG[\"epochs\"] - 1:\n",
    "                val_metrics = evaluate_model(model, test_loader, CONFIG[\"device\"])\n",
    "                \n",
    "                print(f\"--- Epoch {epoch+1}/{CONFIG['epochs']} ---\")\n",
    "                print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "                print(f\"  Val RMSE: {val_metrics['rmse']:.4f}\")\n",
    "                print(f\"  Val MAE: {val_metrics['mae']:.4f}\")\n",
    "                \n",
    "                # Log epoch-specific metrics to MLflow\n",
    "                mlflow.log_metrics({\n",
    "                    f\"train_loss\": train_loss, # Log current epoch loss\n",
    "                    f\"val_rmse\": val_metrics['rmse'],\n",
    "                    f\"val_mae\": val_metrics['mae']\n",
    "                }, step=epoch)\n",
    "                \n",
    "                # Log category-specific metrics for the current epoch\n",
    "                for metric_name, value in val_metrics.items():\n",
    "                    if 'mse' in metric_name or 'mae' in metric_name or 'rmse' in metric_name:\n",
    "                        mlflow.log_metric(f\"{metric_name}\", value, step=epoch) # Log category metrics\n",
    "                \n",
    "                # Save the model state_dict if current validation RMSE is the best so far\n",
    "                if val_metrics['rmse'] < best_val_rmse:\n",
    "                    best_val_rmse = val_metrics['rmse']\n",
    "                    model_save_path = 'best_model.pth'\n",
    "                    torch.save(model.state_dict(), model_save_path)\n",
    "                    mlflow.log_artifact(model_save_path) # Log saved model to MLflow\n",
    "                    print(f\"  Saved best model with Val RMSE: {best_val_rmse:.4f}\")\n",
    "        print(\"Model training complete.\")\n",
    "        \n",
    "        # 8. Final evaluation on the test set\n",
    "        print(\"\\n8. Performing final evaluation on the test set...\")\n",
    "        final_metrics = evaluate_model(model, test_loader, CONFIG[\"device\"])\n",
    "        \n",
    "        print(\"\\nFinal Test Metrics:\")\n",
    "        for metric, value in final_metrics.items():\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "            mlflow.log_metric(f\"final_{metric}\", value) # Log final metrics\n",
    "        print(\"Final evaluation complete.\")\n",
    "        \n",
    "        # 9. Build FAISS indices for efficient item retrieval\n",
    "        print(\"\\n9. Building FAISS indices for each category...\")\n",
    "        faiss_indices = build_faiss_indices(model, item_mappings, CONFIG[\"device\"])\n",
    "        \n",
    "        # Save FAISS indices as artifacts\n",
    "        for category, index in faiss_indices.items():\n",
    "            index_file_path = f'faiss_index_{category}.index'\n",
    "            faiss.write_index(index, index_file_path)\n",
    "            mlflow.log_artifact(index_file_path)\n",
    "            print(f\"Saved FAISS index for '{category}' to '{index_file_path}'.\")\n",
    "        print(\"FAISS indices built and logged.\")\n",
    "        \n",
    "        # 10. Demonstrate sample recommendations\n",
    "        print(\"\\n10. Generating sample recommendations...\")\n",
    "        # Choose a sample user (e.g., the first user in our mapped user IDs)\n",
    "        # Note: In a real application, you'd look up a user's ID from user2idx\n",
    "        sample_user_idx = 0 \n",
    "        \n",
    "        # Reverse item mappings for displaying original business IDs\n",
    "        idx2item_mappings = {}\n",
    "        for category, item_map in item_mappings.items():\n",
    "            idx2item_mappings[category] = {v: k for k, v in item_map.items()}\n",
    "\n",
    "        for category in item_mappings.keys():\n",
    "            if category in faiss_indices: # Ensure FAISS index exists for this category\n",
    "                distances, item_indices = get_recommendations(\n",
    "                    model, sample_user_idx, category, faiss_indices[category], \n",
    "                    item_mappings, CONFIG[\"device\"], k=5\n",
    "                )\n",
    "                \n",
    "                print(f\"\\nTop 5 {category} recommendations for user_idx {sample_user_idx}:\")\n",
    "                for i, (dist, item_idx) in enumerate(zip(distances, item_indices)):\n",
    "                    original_business_id = idx2item_mappings[category].get(item_idx, \"Unknown\")\n",
    "                    print(f\"  {i+1}. Item Index: {item_idx} (Original ID: {original_business_id}), Distance: {dist:.4f}\")\n",
    "            else:\n",
    "                print(f\"Skipping recommendations for category '{category}' as no FAISS index was built.\")\n",
    "        \n",
    "        print(\"\\nPipeline completed successfully! Check MLflow UI for detailed results.\")\n",
    "        \n",
    "        return model, item_mappings, faiss_indices\n",
    "\n",
    "# ============= Execute Pipeline =============\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure all necessary directories exist for saving artifacts if running locally\n",
    "    os.makedirs('mlruns', exist_ok=True) # MLflow will create its own structure, but good practice\n",
    "    \n",
    "    # Setup MLflow tracking\n",
    "    setup_mlflow()\n",
    "    \n",
    "    # Run the main pipeline\n",
    "    # The returned model, item_mappings, and faiss_indices can be used for further inference\n",
    "    # or deployment purposes outside this script.\n",
    "    trained_model, final_item_mappings, final_faiss_indices = main_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a7f92eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>business_city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smOvOajNG0lS4Pq7d8g4JQ</td>\n",
       "      <td>RZtGWDLCAtuipwaZ-UfjmQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>Philadelphia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4Uh27DgGzsp6PqrH913giQ</td>\n",
       "      <td>otQS34_MymijPTdNBoBdCw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>Tucson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IQsF3Rc6IgCzjVV9DE8KXg</td>\n",
       "      <td>eFvzHawVJofxSnD7TgbZtg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>Philadelphia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4hBhtCSgoxkrFgHa4YAD-w</td>\n",
       "      <td>bbEXAEFr4RYHLlZ-HFssTA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>Goleta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EZjT2qJN0mOXypMAqZdSrQ</td>\n",
       "      <td>A2q7d-CBM2-81tVkmS4JMw</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>Reno</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id             business_id  stars primary_category  \\\n",
       "0  smOvOajNG0lS4Pq7d8g4JQ  RZtGWDLCAtuipwaZ-UfjmQ    4.0      Restaurants   \n",
       "1  4Uh27DgGzsp6PqrH913giQ  otQS34_MymijPTdNBoBdCw    4.0      Restaurants   \n",
       "2  IQsF3Rc6IgCzjVV9DE8KXg  eFvzHawVJofxSnD7TgbZtg    5.0      Restaurants   \n",
       "3  4hBhtCSgoxkrFgHa4YAD-w  bbEXAEFr4RYHLlZ-HFssTA    5.0      Restaurants   \n",
       "4  EZjT2qJN0mOXypMAqZdSrQ  A2q7d-CBM2-81tVkmS4JMw    2.0      Restaurants   \n",
       "\n",
       "  business_city  \n",
       "0  Philadelphia  \n",
       "1        Tucson  \n",
       "2  Philadelphia  \n",
       "3        Goleta  \n",
       "4          Reno  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"D:/Projects/Yelp-Recommendation-System/data/raw/yelp_filtered_reviews.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3ec8deda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "primary_category\n",
       "Restaurants    719209\n",
       "Nightlife      189260\n",
       "Bars           168344\n",
       "Shopping       112875\n",
       "beauty_spa      62328\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['primary_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2851ab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['primary_category'] = df['primary_category'].replace('Beauty & Spas', 'beauty_spa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e146041b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>business_city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smOvOajNG0lS4Pq7d8g4JQ</td>\n",
       "      <td>RZtGWDLCAtuipwaZ-UfjmQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>Philadelphia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4Uh27DgGzsp6PqrH913giQ</td>\n",
       "      <td>otQS34_MymijPTdNBoBdCw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>Tucson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IQsF3Rc6IgCzjVV9DE8KXg</td>\n",
       "      <td>eFvzHawVJofxSnD7TgbZtg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>Philadelphia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4hBhtCSgoxkrFgHa4YAD-w</td>\n",
       "      <td>bbEXAEFr4RYHLlZ-HFssTA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>Goleta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EZjT2qJN0mOXypMAqZdSrQ</td>\n",
       "      <td>A2q7d-CBM2-81tVkmS4JMw</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Restaurants</td>\n",
       "      <td>Reno</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id             business_id  stars primary_category  \\\n",
       "0  smOvOajNG0lS4Pq7d8g4JQ  RZtGWDLCAtuipwaZ-UfjmQ    4.0      Restaurants   \n",
       "1  4Uh27DgGzsp6PqrH913giQ  otQS34_MymijPTdNBoBdCw    4.0      Restaurants   \n",
       "2  IQsF3Rc6IgCzjVV9DE8KXg  eFvzHawVJofxSnD7TgbZtg    5.0      Restaurants   \n",
       "3  4hBhtCSgoxkrFgHa4YAD-w  bbEXAEFr4RYHLlZ-HFssTA    5.0      Restaurants   \n",
       "4  EZjT2qJN0mOXypMAqZdSrQ  A2q7d-CBM2-81tVkmS4JMw    2.0      Restaurants   \n",
       "\n",
       "  business_city  \n",
       "0  Philadelphia  \n",
       "1        Tucson  \n",
       "2  Philadelphia  \n",
       "3        Goleta  \n",
       "4          Reno  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "63433615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset saved to: D:/Projects/Yelp-Recommendation-System/data/raw\\yelp_filtered_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "## Finally save the dataset for further use and re-producability\n",
    "\n",
    "import os\n",
    "\n",
    "# Define folder path and create it if it doesn't exist\n",
    "output_dir = DATA_PATH\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = os.path.join(output_dir, 'yelp_filtered_reviews.csv')\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Filtered dataset saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a311bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
